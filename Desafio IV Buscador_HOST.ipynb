{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importa la tabla que parsea numeros de notas a nombre y octava\n",
    "from mido import MidiFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import math\n",
    "import os\n",
    "from contextlib import suppress\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle  \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "midi_notes = pd.read_csv('midi_notes.csv',sep=\";\")\n",
    "midi_scales_chords = pd.read_csv('scales.csv',sep=\";\")\n",
    "midi_scales_full = pd.read_csv('scales_full.csv',sep=\";\")\n",
    "midi_drum_sounds = pd.read_csv('drums_sounds.csv',sep=\";\")\n",
    "midi_drum_sounds.set_index('note', drop=True, inplace=True)\n",
    "midi_drum_sounds.drop('sound',axis=1, inplace=True)\n",
    "midi_drum_sounds_dict = midi_drum_sounds.sound_group.to_dict()\n",
    "midi_instruments = pd.read_csv('instruments.csv',sep=\";\")\n",
    "\n",
    "with open('dict_columns_groups.pkl', 'rb') as df_pkl:\n",
    "        dict_columns_groups = pickle.load(df_pkl)\n",
    "        \n",
    "with open('lgbm_clf.pkl', 'rb') as df_pkl:\n",
    "        LightGBM = pickle.load(df_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Levanta el archivo midi, devuelve cantidad de tracks y mensajes contenidos\n",
    "def load_midi_file(files_mid):\n",
    "    mid = MidiFile(files_mid, clip=False)\n",
    "    print('segundos:', mid.length)\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Itera sobre los mensaje midi de los tracks con mensjaes y genera un dataframe con notas, velocity, tick_start tick_stop\n",
    "def get_theme_df(mid):\n",
    "    dict_notes = {}\n",
    "    dict_notes_end = {}\n",
    "    dict_active_notes = {}\n",
    "    \n",
    "    count_notes = 0\n",
    "    count_notes_end = 0\n",
    "    last_note_on = 0\n",
    "    n_track = 0\n",
    "    n_tracks_used = 0\n",
    "    tempo = 0\n",
    "    tempo_changes = 0\n",
    "    bpm = 0\n",
    "    time_print = 0\n",
    "    count_notes_quantified = 0\n",
    "    \n",
    "    \n",
    "    info = []\n",
    "    controls = []\n",
    "    key_signatures = []\n",
    "    time_signatures = []\n",
    "    \n",
    "    dict_time_signature = {}\n",
    "    dict_time_signature_aux = {}\n",
    "    dict_time_signature_count = 0    \n",
    "\n",
    "    ticks = mid.ticks_per_beat\n",
    "    ticks_quantify = ticks / 8\n",
    "    \n",
    "    for track in mid.tracks:\n",
    "        track_number = 0\n",
    "        track_name = track.name + str(n_track)\n",
    "        info.append(track_name)\n",
    "        n_track = n_track + 1 \n",
    "        if len(track) > 100:      \n",
    "            n_tracks_used = n_tracks_used + 1\n",
    "        time = 0\n",
    "        has_note_off = any(msg.type == 'note_off' for msg in track)\n",
    "        for msg in track:\n",
    "            time = time + msg.time\n",
    "            time_print = round((time) / ticks_quantify, 0) * ticks_quantify\n",
    "            if (msg.type in ['note_on', 'note_off']) and (msg.note > 0):\n",
    "                if (has_note_off and (msg.type == 'note_on')) or (not has_note_off and msg.velocity > 0):\n",
    "                    if (time_print != time):\n",
    "                        count_notes_quantified = count_notes_quantified + 1\n",
    "                        #print(time_print - time)\n",
    "                    dict_notes[count_notes] = {\"note_num\": msg.note, \"start\": time_print, \"velocity\": msg.velocity, \"track_name\": track_number, \"channel\": msg.channel}\n",
    "                    dict_active_notes[msg.note] = time_print\n",
    "                    count_notes = count_notes + 1\n",
    "                    last_note_on = time\n",
    "                else:\n",
    "                    dict_notes_end[count_notes_end] = {\"note_num\": msg.note,\"track_name\": track_number, \"start\": dict_active_notes[msg.note], \"end\": time_print}\n",
    "                    count_notes_end = count_notes_end + 1\n",
    "            else:\n",
    "                if (msg.type == 'control_change'):\n",
    "                    controls.append(msg.value)\n",
    "                elif (msg.type == 'key_signature'):\n",
    "                    key_signatures.append(msg.key)\n",
    "                elif (msg.type == 'time_signature'):\n",
    "                    time_signatures.append(str(msg.numerator) + '/' + str(msg.denominator))\n",
    "                    if (dict_time_signature_count != 0):\n",
    "                        dict_time_signature[dict_time_signature_count] = {\"start\": dict_time_signature_aux[dict_time_signature_count - 1]['start'], \"numerator\": dict_time_signature_aux[dict_time_signature_count - 1]['numerator'], \"denominator\": dict_time_signature_aux[dict_time_signature_count - 1]['denominator'], \"end\": time_print}\n",
    "                    dict_time_signature_aux[dict_time_signature_count] = {\"start\": time_print, \"numerator\": msg.numerator, \"denominator\": msg.denominator}               \n",
    "                    dict_time_signature_count = dict_time_signature_count + 1\n",
    "                    \n",
    "                elif (msg.type == 'program_change'):\n",
    "                    track_number = msg.program\n",
    "                elif (msg.type == 'set_tempo'):\n",
    "                    if (tempo != msg.tempo) and (tempo != 0):\n",
    "                        tempo_changes = 1\n",
    "                    tempo = msg.tempo\n",
    "                    bpm = round(500000*120/msg.tempo,0) \n",
    "    \n",
    "    avg_notes_quantified = count_notes_quantified / count_notes\n",
    "    \n",
    "    tema_df = pd.DataFrame.from_dict(dict_notes, \"index\")    \n",
    "    max_note = tema_df.start.max() + ticks_quantify\n",
    "    dict_time_signature[dict_time_signature_count] = {\"start\": dict_time_signature_aux[dict_time_signature_count - 1]['start'], \"numerator\": dict_time_signature_aux[dict_time_signature_count - 1]['numerator'], \"denominator\": dict_time_signature_aux[dict_time_signature_count - 1]['denominator'], \"end\": max_note}\n",
    "\n",
    "    tema_df_notes_end = pd.DataFrame.from_dict(dict_notes_end, \"index\")\n",
    "    df_time_signature = pd.DataFrame.from_dict(dict_time_signature, \"index\")   \n",
    "    #display(df_time_signature)\n",
    "    df_time_quantify = pd.DataFrame(range(0,max_note.astype(int),int(ticks_quantify)), columns=['start'])\n",
    "\n",
    "    ## Agrega time signature a tema_df\n",
    "    for index, row in df_time_signature.iterrows():\n",
    "        row_start = row.start\n",
    "        row_end = row.end\n",
    "        mask_signature_start = (df_time_quantify.start > row_start) \n",
    "        mask_signature_end = (df_time_quantify.start <= row_end)\n",
    "        df_time_quantify.loc[mask_signature_start & mask_signature_end,'numerator'] = row.numerator\n",
    "        df_time_quantify.loc[mask_signature_start & mask_signature_end,'denominator'] = row.denominator\n",
    "    df_time_quantify.loc[:,'compas_val'] = ticks_quantify / (ticks * df_time_quantify.numerator)    \n",
    "    df_time_quantify.loc[:,'compas_num'] = df_time_quantify.compas_val.cumsum()\n",
    "    \n",
    "    #display(df_time_quantify)    \n",
    "    tema_df = tema_df.join(df_time_quantify[['start','compas_num']].set_index('start'), on='start', how='left')\n",
    "    \n",
    "    tema_df_merged = pd.merge(tema_df, tema_df_notes_end,on=['note_num','start','track_name'])\n",
    "    controls = pd.Series(controls).head(40)\n",
    "    controls = controls[controls > 10].sum() / n_tracks_used\n",
    "    return tema_df_merged, info, controls, key_signatures, time_signatures, n_tracks_used, tempo, bpm, tempo_changes, avg_notes_quantified              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reemplaza outlyers de duración\n",
    "def limit_outlyer_duration_notes(tema_df):\n",
    "    notes_weight = pd.cut(tema_df.duration, 6)\n",
    "\n",
    "    outlyeras_duration = pd.DataFrame(tema_df.duration.quantile([0.05,0.95]))\n",
    "\n",
    "    mask_outlyers_lower = tema_df.duration < outlyeras_duration.duration[0.05]\n",
    "    tema_df.loc[mask_outlyers_lower,'duration'] = outlyeras_duration.duration[0.05]\n",
    "\n",
    "    mask_outlyers_higher = tema_df.duration > outlyeras_duration.duration[0.95]\n",
    "    tema_df.loc[mask_outlyers_higher,'duration'] = outlyeras_duration.duration[0.95]\n",
    "\n",
    "    notes_weight = pd.cut(tema_df.duration, 6)\n",
    "    return tema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theme_stats(file_path, file_name):\n",
    "    ## Instancia el archivo midi\n",
    "    mid = load_midi_file(file_path)\n",
    "    ticks_per_beat = mid.ticks_per_beat\n",
    "    duracion_tema = mid.length\n",
    "    tema_df, info, controls, key_signatures, time_signatures, n_tracks_used, tempo, bpm, tempo_changes, avg_notes_quantified  = get_theme_df(mid)  \n",
    "\n",
    "    ## Toma fraccion de golpe sin el compas\n",
    "    tema_df.compas_num = tema_df.compas_num.fillna(method='ffill')\n",
    "    tema_df.compas_num = tema_df.compas_num.fillna(0)\n",
    "    tema_df.loc[:,'compas_fraction'] = tema_df.compas_num.apply(lambda x: round(x - int(x),3))\n",
    "    tema_df.loc[tema_df.compas_fraction == 1,'compas_fraction'] = 0   \n",
    "    \n",
    "    ## Parsea a enteros los valores numéricos\n",
    "    for col in tema_df.loc[:,tema_df.columns!=\"track_name\"].columns:\n",
    "        tema_df[col] = pd.to_numeric(tema_df[col])\n",
    "    \n",
    "    ## Calcula la duración de cada nota\n",
    "    tema_df.loc[:,'duration'] = tema_df.end - tema_df.start\n",
    "\n",
    "    ## Agregamos informacion de instrumentos y batería\n",
    "    tema_df = pd.merge(tema_df, midi_instruments,how='left',left_on='track_name',right_on='num_mid').drop('num_mid',axis=1)\n",
    "    tema_df.loc[tema_df.channel == 9,['intrument_subcat']] = tema_df[tema_df.channel == 9].note_num.apply(lambda x: midi_drum_sounds_dict[x])\n",
    "    tema_df.loc[tema_df.channel == 9,['intrument_cat']] = 'Drums'\n",
    "    \n",
    "    ## Genera un frame agrupando golpes x el momento del compas\n",
    "    cant_compases = math.trunc(tema_df.compas_num.max()) + 1\n",
    "    print(\"compases:\",cant_compases)\n",
    "    df_compas = tema_df.groupby(['intrument_subcat']).compas_fraction.value_counts() / cant_compases\n",
    "    df_compas = df_compas[df_compas > 0.1]\n",
    "     \n",
    "    ## Generamos datos estadisticos de la instrumentación\n",
    "    instrumentos_por_seg = pd.Series(tema_df.intrument_subcat.value_counts() / duracion_tema)\n",
    "\n",
    "    ## Eliminamos las notas de batería de nuestro analisis musical \n",
    "    tema_df = tema_df.loc[tema_df.intrument_cat != 'Drums']\n",
    "\n",
    "    ## agrega el nombre y octava de notas a la tabla\n",
    "    tema_df = pd.merge(tema_df, midi_notes,how='left',left_on='note_num',right_on='note_number').drop('note_number',axis=1)\n",
    "    \n",
    "    ##elimina notas demasiado cortas y demasiado largas que pueden afectar al análisis\n",
    "    tema_df = limit_outlyer_duration_notes(tema_df)\n",
    "\n",
    "    ## Categoriza duración\n",
    "    tema_df.loc[:,'cat_duration'] = tema_df.duration / mid.ticks_per_beat\n",
    "\n",
    "    ## Categoriza VELOCITY\n",
    "    cat_velocity = pd.cut(tema_df.velocity, 6, labels=['pp','p','m','mf','f','ff'])\n",
    "    tema_df.loc[:,'cat_velocity'] = cat_velocity\n",
    "\n",
    "    ## Describe, muchos de estos valores van a ser utiles como predictores\n",
    "    tema_describe = tema_df.describe()\n",
    "\n",
    "    ## Reemplaza los tiempos de notas muy cercanas por notas simultaneas\n",
    "    #tema_df = cuantize(tema_df)\n",
    "\n",
    "    ## calcula la cantidad de ticks x segundo\n",
    "    ticks_por_seg = tema_df.end.max() / duracion_tema\n",
    "\n",
    "    ## Calcula la cantidad de notas que existen en simultaneo\n",
    "    tema_df_simultaneous =  tema_df.start.value_counts()\n",
    "    tema_df_simultaneous_times = tema_df_simultaneous.loc[tema_df_simultaneous > 1].index.to_list()\n",
    "\n",
    "    #tema_df.note_simultaneous = 0\n",
    "    tema_df.loc[:,'note_simultaneous'] = tema_df.start.apply(lambda x: 1 if x in tema_df_simultaneous_times else 0) \n",
    "    \n",
    "    ## Convierte unidad de medida de timpo Ticks a segundos en cada nota\n",
    "    tema_df.loc[:,'segundo'] = tema_df.start / ticks_por_seg\n",
    "    \n",
    "    ## Shape final del dataset\n",
    "    notas_totales = tema_df.shape[0]\n",
    "\n",
    "    ## indice de actividad (cantidad de notas) por tiempo\n",
    "    cant_eventos_individuales = (notas_totales - len(tema_df_simultaneous_times) / 2)\n",
    "    cant_eventos_piano =  tema_df[tema_df.intrument_cat == \"Piano\"].shape[0]\n",
    "    actividad_por_tiempo = cant_eventos_individuales / duracion_tema\n",
    "    velocity_avg = tema_df.cat_velocity.value_counts(normalize=True)\n",
    "    length_notes_avg = tema_df.cat_duration.value_counts(normalize=True)\n",
    "\n",
    "    ## Analiza proporciones de notas y duraciones mas repetidas\n",
    "    notes_weight = round(tema_df.note.value_counts(normalize=True) * 100,2)\n",
    "\n",
    "    notes_weight = round(tema_df.note_name.value_counts(normalize=True) * 100,2)\n",
    "    all_values_notes = pd.DataFrame(notes_weight).reset_index()\n",
    "    most_probable_scale = all_values_notes.head(7)\n",
    "    scale_coverage = notes_weight.head(7).sum()\n",
    "    avr_vertical_notes = tema_df.note_simultaneous.sum() / notas_totales\n",
    "    cant_pedal_sustain = controls\n",
    "    cant_eventos_por_pedal = cant_eventos_piano / cant_pedal_sustain if cant_pedal_sustain > 5 else np.NaN\n",
    "    cant_pedales_seg = cant_pedal_sustain / duracion_tema if cant_pedal_sustain > 5 else np.NaN\n",
    "\n",
    "    #obtiene informacion de la escala\n",
    "    nombre_escala = pd.merge(most_probable_scale, midi_scales_chords, how='left', left_on='index', right_on='note_name')\n",
    "    nombre_escala.fillna(0,inplace=True)\n",
    "    nombre_escala_T = nombre_escala.T\n",
    "    nombre_escala_T\n",
    "    mask = nombre_escala_T.apply(lambda x: True if all(x != 0) else False, axis=1)\n",
    "    mask\n",
    "    tabla_esacla = nombre_escala_T[mask].T\n",
    "    tabla_esacla\n",
    "\n",
    "    nombre_columna_Tmaj = tabla_esacla.columns[3]\n",
    "    tonalidad  = 0\n",
    "    tonalidad_escala = 'M'\n",
    "    dic = {6:1, 7:2, 1:3, 2:4, 3:5, 4:6, 5:7}\n",
    "    if nombre_columna_Tmaj != \"U\":\n",
    "        tabla_esacla.set_index(tabla_esacla.columns[3],inplace=True,drop=False)\n",
    "        mayor_chord_coverage = tabla_esacla.loc[[1,3,5],:'note_name_x'].sum()[1]\n",
    "        minor_chord_coverage = tabla_esacla.loc[[6,1,3],:'note_name_x'].sum()[1]\n",
    "\n",
    "\n",
    "        if minor_chord_coverage > mayor_chord_coverage:\n",
    "            tonalidad = tabla_esacla.iloc[0,2]\n",
    "            # reemplazo el 6to grado por la tónica\n",
    "             # define desired replacements here\n",
    "\n",
    "            tabla_esacla[nombre_columna_Tmaj] = tabla_esacla[nombre_columna_Tmaj].apply(lambda x: dic[x])\n",
    "            tabla_esacla\n",
    "            tonalidad_escala = 'm'\n",
    "            tabla_esacla.set_index(nombre_columna_Tmaj,inplace=True,drop=False)\n",
    "            tabla_esacla.sort_index(inplace=True)\n",
    "        else:\n",
    "            tonalidad = nombre_columna_Tmaj\n",
    "\n",
    "    elif len(key_signatures) > 0:\n",
    "        if 'b' in key_signatures[0]:\n",
    "            dict_keys = {'Db':'C#', 'Eb':'D#', 'Gb':'F#', 'Ab':'G#', 'Bb':'A#','Dbm':'C#m', 'Ebm':'D#m', 'Gbm':'F#m', 'Abm':'G#m', 'Bbm':'A#m'}\n",
    "            tonalidad =  dict_keys[key_signatures[0]]\n",
    "        else:\n",
    "            tonalidad = key_signatures[0]\n",
    "\n",
    "        midi_scales_chords_weighted = pd.merge(midi_scales_chords[['note_name', tonalidad]], all_values_notes, how='left', left_on='note_name', right_on='index',)\n",
    "        midi_scales_chords_weighted.drop('index',axis=1,inplace=True)\n",
    "        midi_scales_chords_weighted.columns = ['note_name', 'scale', 'weight']\n",
    "        midi_scales_chords_weighted.set_index(midi_scales_chords_weighted.columns[1],inplace=True,drop=False)\n",
    "        if 'm' in key_signatures[0]:\n",
    "            mayor_chord_coverage = midi_scales_chords_weighted.loc[[3,5,7],'weight'].sum()\n",
    "            minor_chord_coverage = midi_scales_chords_weighted.loc[[1,3,5],'weight'].sum()\n",
    "            tonalidad = tonalidad.replace('m','')\n",
    "            tonalidad_escala = 'm'\n",
    "        else:\n",
    "            mayor_chord_coverage = midi_scales_chords_weighted.loc[[1,3,5],'weight'].sum()\n",
    "            minor_chord_coverage = midi_scales_chords_weighted.loc[[6,1,3],'weight'].sum()        \n",
    "            if minor_chord_coverage > mayor_chord_coverage:\n",
    "                tonalidad_escala = 'm'\n",
    "                midi_scales_chords_weighted_mask = midi_scales_chords_weighted.scale > 0\n",
    "                midi_scales_chords_weighted.loc[midi_scales_chords_weighted_mask, 'scale'] = midi_scales_chords_weighted.loc[midi_scales_chords_weighted_mask, 'scale'].apply(lambda x: dic[x])\n",
    "        midi_scales_chords_weighted.sort_index(inplace=True)\n",
    "        midi_scales_chords_weighted.drop('scale',axis=1,inplace=True)\n",
    "        scale_coverage = midi_scales_chords_weighted.head(7).sum()[1]\n",
    "        tabla_esacla = midi_scales_chords_weighted\n",
    "        tabla_esacla\n",
    "    else:\n",
    "        tonalidad = 'U'\n",
    "        tonalidad_escala = 'U'\n",
    "        tabla_esacla = pd.DataFrame([ [ 0 for y in range( 2 ) ] for x in range( 7 ) ],columns=['1','2'])\n",
    "        tabla_esacla.iloc[0:7,:] = 0\n",
    "        mayor_chord_coverage = 0.5\n",
    "        minor_chord_coverage = 0.5\n",
    "\n",
    "    time_signatures_fix = time_signatures    \n",
    "    if (len(time_signatures) == 0):\n",
    "        time_signatures_fix = \"\"\n",
    "        \n",
    "    midi_scale_full = midi_scales_full.set_index('note_name', inplace=True,drop=False)    \n",
    "    print(tonalidad, tonalidad_escala)\n",
    "    midi_scale_full = midi_scales_full.loc[:,[tonalidad]]\n",
    "    midi_scale_full.columns = ['nota_relativa']\n",
    "    tema_df = pd.merge(tema_df, midi_scale_full,on=['note_name']) \n",
    "    \n",
    "    ## Calcula la cantidad de notas que existen en simultaneo por instrumento\n",
    "    ## Calcula apariciones de acordes por compas\n",
    "    dict_sim_notes_by_instrument_cat = {}\n",
    "    list_chords = []\n",
    "    for instrument_cat in midi_instruments.intrument_cat.unique():\n",
    "        mask_cat_instrument = tema_df.intrument_cat == instrument_cat\n",
    "        df_instrument = tema_df[mask_cat_instrument]\n",
    "        if (df_instrument.shape[0] > 0):\n",
    "            ## Calcula la cantidad de notas que existen en simultaneo\n",
    "            tema_df_simultaneous =  df_instrument.start.value_counts()\n",
    "            tema_df_simultaneous_times = tema_df_simultaneous.loc[tema_df_simultaneous > 1].index.to_list()\n",
    "            notes_simul_by_instrument_mask = df_instrument.start.apply(lambda x: True if x in tema_df_simultaneous_times else False)\n",
    "            avg_notes_simul_by_instrument = notes_simul_by_instrument_mask.sum() / df_instrument.shape[0]\n",
    "            dict_sim_notes_by_instrument_cat['avg_simult_'+instrument_cat] = avg_notes_simul_by_instrument\n",
    "\n",
    "            ## Calcula apariciones de acordes por compas\n",
    "            chords_by_inst = df_instrument[notes_simul_by_instrument_mask].groupby(['start']).nota_relativa.unique()\n",
    "            chords_by_inst_trnasform = chords_by_inst.reset_index().nota_relativa.apply(lambda x: '_'.join( np.sort(x) ) if len(x) > 2 else np.NaN)#.value_counts().dropna()\n",
    "            list_chords.extend(chords_by_inst_trnasform.tolist())\n",
    "    \n",
    "    chords_series = pd.Series(list_chords).dropna().value_counts()\n",
    "    head_chord_series = chords_series.head(5) / cant_compases\n",
    "    \n",
    "    cant_dist_chords = len(chords_series)\n",
    "    print(cant_dist_chords)   \n",
    "            \n",
    "            \n",
    "            \n",
    "    df_sim_notes_by_instrument = pd.DataFrame.from_dict(dict_sim_notes_by_instrument_cat, \"index\")\n",
    "\n",
    "    ## crea el Music stats    \n",
    "    music_stats = pd.DataFrame(columns=['first_time_signature', 'cant_time_signatures', 'bpm', 'compases', 'cant_dist_chords',\n",
    "                                        'avg_notes_quantified', 'tempo_changes', 'tonalidad',\n",
    "                                        'tonalidad_escala','scale_coverage','mayor_chord_coverage','minor_chord_coverage',\n",
    "                                          'scale_note_avg_1','scale_note_avg_2','scale_note_avg_3','scale_note_avg_4',\n",
    "                                        'scale_note_avg_5','scale_note_avg_6','scale_note_avg_7', 'avr_simult_notes',\n",
    "                                        'cant_eventos_por_pedal', 'cant_pedales_seg','duracion_seg','tracks_used', 'info_tracks'])\n",
    "    music_stats.loc[0] = [time_signatures[0], len(time_signatures), bpm, cant_compases, \n",
    "                          cant_dist_chords, avg_notes_quantified, tempo_changes, tonalidad, \n",
    "                          tonalidad_escala, scale_coverage,  mayor_chord_coverage, minor_chord_coverage, \n",
    "                           tabla_esacla.iloc[0,1], tabla_esacla.iloc[1,1], tabla_esacla.iloc[2,1], \n",
    "                           tabla_esacla.iloc[3,1], tabla_esacla.iloc[4,1], tabla_esacla.iloc[5,1], \n",
    "                           tabla_esacla.iloc[6,1] if tabla_esacla.shape[0] > 6 else 0, avr_vertical_notes,cant_eventos_por_pedal,\n",
    "                           cant_pedales_seg,duracion_tema,n_tracks_used,\n",
    "                          \" \".join(info)]\n",
    "      \n",
    "    tema_describe = tema_df.describe()\n",
    "    \n",
    "    data_describe = pd.DataFrame(tema_describe.loc[tema_describe.index != 'count',['note_num','Octave','duration']].unstack())\n",
    "    data_describe.reset_index(inplace=True)\n",
    "    data_describe.loc[:,'name'] = data_describe.level_0 + \"_\" + data_describe.level_1\n",
    "    data_describe.set_index('name',inplace=True)\n",
    "    data_describe.drop(['level_0','level_1'],axis=1, inplace=True)\n",
    "    \n",
    "    ## Agrego informacion de genero y grupo tomado de los archivos\n",
    "    array_dir = file_path.split('\\\\')\n",
    "    labels_mid = pd.DataFrame(data=[[array_dir[0],array_dir[1],'analizar_midi']], columns=['Genero','Grupo','tema']).T\n",
    "    instrumentos_por_seg = instrumentos_por_seg.add_prefix('inst_')\n",
    "    data_describe = data_describe[data_describe.columns[0]].add_prefix('describe_')\n",
    "    length_notes_avg = length_notes_avg.add_prefix('length_note_')\n",
    "    velocity_avg = velocity_avg.add_prefix('velocity_cat_')\n",
    "    head_chord_series = head_chord_series.add_prefix('chord_')\n",
    "    \n",
    "    df_final = pd.concat([music_stats.T,df_sim_notes_by_instrument, instrumentos_por_seg, data_describe,velocity_avg,length_notes_avg, labels_mid, df_compas, head_chord_series])\n",
    "    return df_final.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_midi_from_path():\n",
    "    path_midi = 'GET_FILE'\n",
    "\n",
    "    # the dictionary to pass to pandas dataframe\n",
    "    dict = {}\n",
    "    count_files = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(path_midi, topdown=False):\n",
    "        for name_file in files:\n",
    "            dict[count_files] = {\"file\": os.path.join(root, name_file), \"file_name\": name_file}\n",
    "            count_files = count_files + 1\n",
    "\n",
    "    df_files_analize = pd.DataFrame.from_dict(dict, \"index\").iloc[0,:]\n",
    "    display(df_files_analize)\n",
    "    \n",
    "    return get_theme_stats(df_files_analize.file, df_files_analize.file_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_midi = pd.read_csv('data_clean_new.csv',sep=\",\")\n",
    "data_midi.drop('Unnamed: 0', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciamos nuestra API\n",
    "import requests\n",
    "from flask import  Flask, request, jsonify, render_template\n",
    "app = Flask('recomendar canciones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/related_songs\",methods=['GET'])\n",
    "def get_related_songs():   \n",
    "    \n",
    "    search=request.args['search']\n",
    "    duracion_notas =float(request.args['duracion_notas'])\n",
    "    amplitud_tonal = float(request.args['amplitud_tonal'])\n",
    "    ritmica_instrument = float(request.args['ritmica_instrument']) \n",
    "    ritmica_drums = float(request.args['ritmica_drums'])\n",
    "    armonia = float(request.args['armonia'])\n",
    "    dinamica = float(request.args['dinamica'])\n",
    "    instrumentacion = float(request.args['instrumentacion'])\n",
    "    tempo = float(request.args['tempo'])\n",
    "    notas_simultaneas = float(request.args['notas_simultaneas']) \n",
    "    duracion_tema = float(request.args['duracion_tema']) \n",
    "    cant_resultados = int(request.args['cant_resultados'])\n",
    "    \n",
    "    new_midi = \"\"\n",
    "    if (search == 'False'):\n",
    "        new_midi = get_midi_from_path()\n",
    "        data = pd.concat([data_midi, new_midi]) \n",
    "    else:\n",
    "        data = data_midi\n",
    "        \n",
    "        \n",
    "    data.set_index(data.tema,inplace=True,drop=False)\n",
    "    data = data.iloc[:,0:len(data_midi.columns)]\n",
    "    data.fillna(0, inplace=True)\n",
    "    X=data.drop([\"Genero\",'tema'],axis=1)\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    X__sc = sc.fit_transform(X)\n",
    "\n",
    "    df_scaled = pd.DataFrame(X__sc,columns=X.columns)\n",
    "    df_scaled.set_index(data.tema,inplace=True)\n",
    "    df_scaled\n",
    "    \n",
    "    mask_category = True\n",
    "    if (search == 'False'):\n",
    "        new_midi = df_scaled.iloc[data.shape[0] - 1:data.shape[0],:]\n",
    "        new_category = LightGBM.predict(new_midi)\n",
    "        print('predicted label:',new_category)\n",
    "        mask_category = data.Genero == new_category[0]\n",
    "\n",
    "    dict_key_values = {'instrumentacion':instrumentacion,\n",
    "                     'ritmica_drums':ritmica_drums ,\n",
    "                     'ritmica_instrument':ritmica_instrument ,\n",
    "                     'amplitud_tonal':amplitud_tonal ,\n",
    "                     'dinamica':dinamica, \n",
    "                     'duracion_notas1':duracion_notas, \n",
    "                     'duracion_notas2':duracion_notas, \n",
    "                     'notas_simultaneas':notas_simultaneas ,\n",
    "                     'tempo':tempo, \n",
    "                     'duracion_tema':duracion_tema ,\n",
    "                     'armonia1':armonia, \n",
    "                     'armonia2':armonia, \n",
    "                     'armonia3':armonia, \n",
    "                     'armonia4':armonia, \n",
    "                     'armonia5':armonia}\n",
    "\n",
    "    for keys in dict_key_values.keys():\n",
    "        cant_keys = len(dict_columns_groups[keys])\n",
    "        for column in dict_columns_groups[keys]:\n",
    "            df_scaled[column] = (df_scaled[column] * cant_keys ) * (1 / (dict_key_values[keys] + 0.1) )\n",
    "\n",
    "    similarity = cosine_similarity(X__sc)\n",
    "    df_similarity = pd.DataFrame(similarity, columns=data.tema,index=data.tema)\n",
    "    \n",
    "    if (search == 'False'):\n",
    "        search_song = 'analizar_midi'\n",
    "    else:\n",
    "        search_song = data[data['tema'].str.contains(search)].tema.iloc[0]\n",
    "        search_song\n",
    "\n",
    "    id_song = df_similarity.loc[mask_category,search_song].sort_values(ascending=False)\n",
    "    id_song_result = id_song.iloc[1:cant_resultados + 1:]\n",
    "    display(id_song_result)\n",
    "\n",
    "    return id_song_result.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"recomendar canciones\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file         GET_FILE\\ACCUSTOM.MID\n",
       "file_name             ACCUSTOM.MID\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segundos: 180.7116743499994\n",
      "compases: 64\n",
      "D# M\n",
      "43\n",
      "predicted label: ['Jazz']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tema\n",
       "PRELUD~1.MID    0.756382\n",
       "WHENSU~1.MID    0.692318\n",
       "ASKMENOW.MID    0.689100\n",
       "MYONEA~1.MID    0.674190\n",
       "CRYMEA~1.MID    0.663884\n",
       "Name: analizar_midi, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Sep/2020 09:28:15] \"\u001b[37mGET /related_songs?search=False&cant_resultados=5&instrumentacion=20&ritmica_drums=20&ritmica_instrument=15&amplitud_tonal=3&dinamica=4&duracion_notas=1&notas_simultaneas=2&tempo=10&duracion_tema=15&armonia=4 HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file         GET_FILE\\Waterloo.mid\n",
       "file_name             Waterloo.mid\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segundos: 161.05262687498774\n",
      "compases: 100\n",
      "D M\n",
      "57\n",
      "predicted label: ['ClassicRock']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tema\n",
       "How_Can_You_Mend_A_Broken_H    0.526958\n",
       "Poor_Little_Fool               0.509244\n",
       "Its_Raining_Again              0.490952\n",
       "Valleri                        0.490888\n",
       "Time_in_a_Bottle               0.481791\n",
       "Name: analizar_midi, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Sep/2020 09:34:09] \"\u001b[37mGET /related_songs?search=False&cant_resultados=5&instrumentacion=20&ritmica_drums=20&ritmica_instrument=15&amplitud_tonal=3&dinamica=4&duracion_notas=1&notas_simultaneas=2&tempo=10&duracion_tema=15&armonia=4 HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file         GET_FILE\\Malena.mid\n",
       "file_name             Malena.mid\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segundos: 100.72000000000061\n",
      "compases: 42\n",
      "G m\n",
      "25\n",
      "predicted label: ['TANGO']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tema\n",
       "malena      0.760933\n",
       "Cuartito    0.729880\n",
       "candil      0.646502\n",
       "Alaluzde    0.646502\n",
       "elultimo    0.588872\n",
       "Name: analizar_midi, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Sep/2020 09:37:11] \"\u001b[37mGET /related_songs?search=False&cant_resultados=5&instrumentacion=20&ritmica_drums=20&ritmica_instrument=15&amplitud_tonal=3&dinamica=4&duracion_notas=1&notas_simultaneas=2&tempo=10&duracion_tema=15&armonia=4 HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file         GET_FILE\\Blink_182_-_Josie.mid\n",
       "file_name             Blink_182_-_Josie.mid\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segundos: 198.44745359999362\n",
      "compases: 167\n",
      "C M\n",
      "4\n",
      "predicted label: ['Punk']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tema\n",
       "Blink_182_-_Josie                    0.582707\n",
       "Green_Day_-_Pulling_Teeth            0.461529\n",
       "Green_Day_-_2000_Light_Years_Away    0.442249\n",
       "The_Offspring_-_Genocide             0.389138\n",
       "mxpx_-_Chickmagnet                   0.330692\n",
       "Name: analizar_midi, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Sep/2020 09:38:37] \"\u001b[37mGET /related_songs?search=False&cant_resultados=5&instrumentacion=20&ritmica_drums=20&ritmica_instrument=15&amplitud_tonal=3&dinamica=4&duracion_notas=1&notas_simultaneas=2&tempo=10&duracion_tema=15&armonia=4 HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file         GET_FILE\\BABELS.MID\n",
       "file_name             BABELS.MID\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segundos: 147.34771199999292\n",
      "compases: 155\n",
      "G# M\n",
      "89\n",
      "predicted label: ['ClassicRock']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tema\n",
       "Celebration          0.673809\n",
       "OneAfter909          0.627890\n",
       "Slow_Hand            0.548573\n",
       "Love_and_Marriage    0.547081\n",
       "NotASecondTime       0.512337\n",
       "Name: analizar_midi, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Sep/2020 09:40:29] \"\u001b[37mGET /related_songs?search=False&cant_resultados=5&instrumentacion=20&ritmica_drums=20&ritmica_instrument=15&amplitud_tonal=3&dinamica=4&duracion_notas=1&notas_simultaneas=2&tempo=10&duracion_tema=15&armonia=4 HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file         GET_FILE\\683-SEXY.MID\n",
       "file_name             683-SEXY.MID\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segundos: 694.2338530312707\n",
      "compases: 150\n",
      "U U\n",
      "0\n",
      "predicted label: ['ClassicRock']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tema\n",
       "How_Do_You_Do_It    0.627864\n",
       "Its_Not_Unusual     0.624163\n",
       "Georgie_Girl        0.599594\n",
       "Revolution          0.500306\n",
       "Revolution1         0.486432\n",
       "Name: analizar_midi, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Sep/2020 09:41:53] \"\u001b[37mGET /related_songs?search=False&cant_resultados=5&instrumentacion=20&ritmica_drums=20&ritmica_instrument=15&amplitud_tonal=3&dinamica=4&duracion_notas=1&notas_simultaneas=2&tempo=10&duracion_tema=15&armonia=4 HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de los temas recomendados\n",
    "mask_zero_columns = X.loc[id_song.head(1).index[0],:] == 0\n",
    "display(X.loc[id_song.head(cant_resultados + 1).index,~mask_zero_columns].T.iloc[0:50,:])\n",
    "display(X.loc[id_song.head(cant_resultados + 1).index,~mask_zero_columns].T.iloc[50:100,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
